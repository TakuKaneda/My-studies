---
title: "5. Introduction to Bayesian Computation"
output:
  html_document:
    df_print: paged
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


### Import the library
```{r}
library(LearnBayes)
library(kableExtra)  # for display table in HTML format
```

### 5.1 Introduction
The two way for summarization of posterior:  
- Direct `R` function if the sampling density has a familiar functional form.
- Brute-force where one computes value of the posterior on a grid. 

In this chapter, we deal with more sophisticated methods for *computational problems*.

### 5.2 Computing Integrals
The Bayesian recipe for inference is conceptually simple. If we observe data $y$ from a sampling density $f(y|\theta)$, where $\theta$ is a parameter vector and assign the prior $g(\theta)$. Then the posterior is proportional to:
$$
g(\theta|y) \propto g(\theta)f(y|\theta).
$$
*The computational problems is to summarize this multivariate pdf to perform inference about functions of $\theta$*. 

1) The posterior mean of a function $h(\theta)$:
$$
E(h(\theta|y)) \propto \frac{\int h(\theta)g(\theta)f(y|\theta)d\theta}{\int g(\theta)f(y|\theta)d\theta}
$$
2) The posterior prob that $h(\theta)$ falls in a set $A$:
$$
P(h(\theta)\in A|y) = \frac{\int_{h(\theta)\in A} g(\theta)f(y|\theta)d\theta}{\int g(\theta)f(y|\theta)d\theta}
$$
3) Marginal posterior of density $\theta_1$ by integrating a nuisance param $\theta_2$:
$$
g(\theta_1|y) \propto \int g(\theta_1,\theta_2|y) d\theta_2 
$$
There are number of **quadrature** methods are available, however they are limited in terms of 1) the choice of quadrature depends on the location and shape of the post 2)  the number of evaluation grows exponentially. Hence, we focus on the methods for computing integrals which are applicalble to high-dim Bayesian problems.


### 5.3 Setting Up a Problem in `R`
Suppose one is able to express an explicity expression for the joint posterior. We don't have to include any normalizaing constants that don't contain the params. It is usefult to *reparameterize* all params so that the are all **real-valued**. For example, if one is interested in a proportion $p$, then we can transform by the logit: $\text{logit}(p) = \log(p/(1-p))$. 

The log of posterior can be expressed in a following grneral way.
```
mylogposterior = function(theta,data){
[statements that compute the log density]
return(val)
}
```

*Assumptions*  
- `theta`, ie $\theta$ is $k$-dim vector.
- `data`, $y$ is a vec of observed values or a list of data values and other model specifications such as the prior hyperparams. 

Suppose we observe data $y=(y_1,...,y_n)$ from a sampling density $f(y|\theta)$ and one assigns the prior $g(\theta)$. The log of the post density of $\theta$ is given by (ignoring an additive constant):
$$
\log g(\theta|y) = \log g(\theta) + \sum_{i=1}^n \log f(y_i|\theta)
$$
**Gaussian case**: suppose we sample from a normal distribution with mean $\mu$ and std $\sigma$. the param vector is $\theta = (\mu, \log \sigma)$ (to be real-valued). We place the prior $N(10,20)$ on the mean $\mu$ and a flat prior on $\log \sigma$. Then the post would be 
$$
\log g(\theta|y) = \log \phi(\mu;10,20) + \sum_{i=1}^n \log f(y_i|\mu,\sigma)
$$
where $\phi(y;\mu,\sigma)$ is the normal pdf with mean $\mu$ and std $\sigma$. The likelihood can be expressed as:
```{r}
logf = function(y, mu, sigma)
  dnorm(y, mean=mu, sd=sigma,log = TRUE)
```
If we have $n$ data, simply by summing all likelihoods:
```
sum(logf(data,mu,sigma))
```
Then, the function `mylogposterior` can be defined as follows:
```{r}
mylogposterior = function(theta, data){
  n = length(data) # num of data
  mu = theta[1]; sigma = theta[2] # retrieve theta vals
  logf = function(y, mu, sigma){  # def of likelihood on a data
    dnorm(y, mean=mu, sd=sigma,log = TRUE)
  }
  val = dnorm(mu, mean = 10, sd = 20, log = TRUE) + sum(logf(data,mu,sigma)) # compute the log post
  return(val)
}
```

### 5.4 A Beta-Binomial Model for Overdispersion
Tsutakawa et al (1985) describe the problem of simultaneously estimationg the rates of death from stomach cancer for males at risk in the age of 45-64 for the largest cities in Missouri. The following table shows the mortality rates for 20 of these cities; $n$ the num at risk and $y$ the num of cancer deaths. 
```{r}
data(cancermortality)
attach(cancermortality)
kable(cancermortality)%>%
kable_styling(full_width = F)
```
A first model would be {$y_j$} represents indepedent binomial samples with sample size {$n_j$} and a fixed prob $p$. But one notices that the data is *overdispersed* in the sense that {$y_j$} display more variation than that of binomial model. Thus we apply **beta-binomial** model that assiuming $y_j$ is from a binomial distribution with a parameter $p$ that has a beta distribution with mean $\eta$ and precision $K$  (https://en.wikipedia.org/wiki/Beta-binomial_distribution).  

$$
f(y_j|\mu,K) = {n_j \choose y_j} \frac{B(K\eta+y_j,K(1-\eta)+n_j-y_j)}{B(K\eta,K(1-\eta))}.
$$

Note that $ K = \alpha + \beta, \eta = \alpha/(\alpha + \beta)$ of Wikipedia definition. Suppose we assign the vague prior:

$$
g(\eta,K) \propto \frac{1}{\eta(1-\eta)}\frac{1}{(1+K)^2}.
$$
Then the posterior of the params is given by
$$
g(\eta,K|\text{data}) \propto \frac{1}{\eta(1-\eta)}\frac{1}{(1+K)^2} \prod_{j=1}^{20} \frac{B(K\eta+y_j,K(1-\eta)+n_j-y_j)}{B(K\eta,K(1-\eta))}
$$
where $0<\eta<1,K>0$.  

Let's do the process in `R`.
```{r}
betabinexch0 = function(theta, data){
  eta = theta[1]; K = theta[2]  # retrieve params
  y = data[,1]; n = data[,2] # data vactors
  N = length(y)
  logf = function(y,n,K,eta){  # log of the likelihood
    lbeta(K*eta + y, K*(1-eta)+n-y) - lbeta(K*eta,K*(1-eta)) # lbeta: log of the beta function
  }
  like = sum(logf(y,n,K,eta))  # sum of likelihoods
  val = like - 2*log(1+K) - log(eta) - log(1-eta)  # log val of post 
  return(val)
}
```
See a contour plot of the posterior.
```{r}
mycontour(betabinexch0,c(.0001,.003,1,20000),cancermortality,xlab="eta",ylab="K")
```
We observe the **strong skewness** in the density especially for $K$. 

Following the guide of Section 5.3, we transform the params to the real-line.
$$
\theta_1 = \text{logit}(\eta) = \log(\frac{\eta}{1-\eta})\\
\theta_2 = \log(K)
$$
The posterior is:
$$
g_1(\theta_1,\theta_2|\text{data}) = g(\frac{e^{\theta_1}}{1+e^{\theta_1}},e^{\theta_2})\frac{e^{\theta_1+\theta_2}}{(1+e^{\theta_1})^2}
$$
where the right term of the product is the Jacobian:
$$
\left| \frac{\partial(\eta,K)}{\partial(\theta_1,\theta_2)} \right| = \left|\begin{bmatrix}
\frac{\partial \eta}{\partial \theta_1} & \frac{\partial \eta}{\partial \theta_2}\\
\frac{\partial K}{\partial \theta_1} & \frac{\partial K}{\partial \theta_2}
\end{bmatrix} \right|
=\left| \begin{bmatrix}
\frac{e^{\theta_1}}{(1+e^{\theta_1})^2} & 0\\
0 & e^{\theta_2}
\end{bmatrix} \right|  \\
= \frac{e^{\theta_1+\theta_2}}{(1+e^{\theta_1})^2}
$$

The corresponding posterior can be defined as follows `betabinexch`:
```{r}
betabinexch = function(theta, data){
  eta = exp(theta[1])/(1+exp(theta[1])); K = exp(theta[2])
  y = data[,1]; n = data[,2] # data vactors
  N = length(y)
  logf = function(y,n,K,eta){  # log of the likelihood
    lbeta(K*eta + y, K*(1-eta)+n-y) - lbeta(K*eta,K*(1-eta)) # lbeta: log of the beta function
  }
  like = sum(logf(y,n,K,eta))  # sum of likelihoods
  val = like + theta[2] - 2*log(1+exp(theta[2]))  # log val of post 
  return(val)
}
```
Plot the contour.
```{r}
mycontour(betabinexch, c(-8,-4.5, 3, 16.5), cancermortality, xlab="logit eta", ylab = "log K")
```
Observing that the density has an unusual shape, the strong skewness has been reduced and distribution is more amenable to the computational methods in the followings. 


### 5.5 Approximates Based on Posterior Models
One method of summarizing a multivariate posterior distribution is based on the behavior of the density about its **mode**. Let $\theta$ be a param vector with prior $g(\theta)$, and suppose we observe data $y$ with sampling density $f(y|\theta)$. Then consider the log of the joint density:
$$
h(\theta,y) = \log (g(\theta)f(y|\theta)) = h(\theta)
$$
Note that since data $y$ is not random, we simply consider $h$ as the function of $\theta$. 

Denoting the posterior mode of $\theta$ by $\hat \theta$ (hence $h(\hat \theta) = 0$). Then the second-order Taylor approximation can be expressed as:
$$
h(\theta) \approx h(\hat \theta) + (\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)
$$
where $h''(\hat \theta)$ is the Hessian evaluated at the mode. The post density is approximated by a multivariate normal density with mean $\hat \theta$ and variance-covariance matrix:
$$
V = (- h''(\hat \theta))^{-1}
$$
It means:
$$
g(\theta|y) \propto g(\theta)f(y|\theta)\\
g(\theta|y) \propto \exp h(\theta) \propto \exp[\frac{1}{2}(\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)]\\
= \exp[-\frac{1}{2}(\theta-\hat \theta)^T (\{-h''(\hat \theta)\}^{-1})^{-1}(\theta-\hat \theta)]\\
= \exp[-\frac{1}{2}(\theta-\hat \theta)^T V^{-1}(\theta-\hat \theta)]\\
$$
In addition, we can derive the prior predictive by integrate out $\theta$:
$$
f(y) \approx (2\pi)^{d/2}g(\hat \theta)f(y|\hat\theta)|-h''(\hat\theta)|^{-1/2}
$$
where $d$ is dim of $\theta$.
**Proof**  
The prior predictive distribution $f(y)$ can be given as:
$$
f(y) = \int g(\theta)f(y|\theta)d\theta = \int \exp(h(\theta))d\theta\\
\approx \int \exp \{h(\hat \theta) + (\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)\}d\theta\\
= \exp\{h(\hat \theta)\}\int \exp \{(\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)\}d\theta
$$
we have that $\exp\{h(\hat \theta)\} = g(\hat\theta)f(y|\hat\theta)$. Also from the def of multivariate normal density:
$$
1 = \int \frac{\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}{\sqrt{(2\pi)^k|\Sigma|}}dx \\
\iff (2\pi)^{k/2}|\Sigma|^{1/2} = \int {\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}dx
$$
since we have that
$$
\exp[-\frac{1}{2}(\theta-\hat \theta)^T (\{-h''(\hat \theta)\}^{-1})^{-1}(\theta-\hat \theta)]
= \exp[-\frac{1}{2}(\theta-\hat \theta)^T V^{-1}(\theta-\hat \theta)]
$$
One can derive
$$
\int \exp \{(\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)\}d\theta \\
= \int \exp[-\frac{1}{2}(\theta-\hat \theta)^T V^{-1}(\theta-\hat \theta)] d\theta\\
= (2\pi)^{d/2}|V|^{1/2} = (2\pi)^{d/2}|(-h''(\hat\theta))^{-1}|^{1/2}\\
=  (2\pi)^{d/2}|-h''(\hat\theta)|^{-1/2}
$$
Thus 
$$
f(y) =  \exp\{h(\hat \theta)\} \cdot\int \exp \{(\theta-\hat \theta)^T h''(\hat \theta)(\theta-\hat \theta)\}d\theta\\
= g(\hat\theta)f(y|\hat\theta) \cdot  (2\pi)^{d/2}|-h''(\hat\theta)|^{-1/2}
$$
*(Q.E.D)*

To apply this approximation, we need to find the mode of the posterior $\hat \theta$. One method is *Newton's method*. The estimation of the mode at $t$-th iteration $\theta^t$ is given by
$$
\theta^t = \theta^{t-1}-[h''(\theta^{t-1})]^{-1}h'(\theta^{t-1})
$$
where $\theta^{t-1}$ is the previous value and $\theta^0$ is an initial value. Other approach is the *Nelder-Mead* algorithm (https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) which is the default method of `R` function `optim`. This algorithm is an iterative method based on the evaluation of the objective over verticeis of simplex. In the following we use Nelder-Mead since it is less sensitive to the choice of starting value compared to Newton's.

After evaluating the log posterior density, the function `laplace` in `LearnBayes` will compute the joint posterior mode by using `optim`. Let's see them with the follwing example.

### 5.6 Example
We test with beta-binomial modeling example. Based on our contour plot, we start the Nelder-Mead method with initial guess of $(\text{logit}(\eta),\log K)= (7,6)$.

```{r}
fit = laplace(betabinexch,c(-7,6),cancermortality)
fit
```
The result shows that the post mode to be (-6.82,7.58). From the optput of `laplace`, we have the approx that $(\text{logit}(\eta),\log K)$ follows a bivariate normal with mean vector `fit$mode` and covariance matrix `fit$var`. The following figure shows the contour of this density:
```{r}
npar = list(m=fit$mode,v=fit$var)
mycontour(lbinorm,c(-8,-4.5,3,16.5),npar,xlab="logit eta",ylab="log K")
```
We see the big difference between the exact (section 5.4) and approxed normal posteriors. 

One advantage of MN-algorithm is *one obtains quick summaries of the parameters by the multivariate normal approx*. One can constract approx prob interval from diag of the covariance matrix. For instance 90%  prob interval for the params:
```{r}
se = sqrt(diag(fit$var))
fit$mode-1.645*se  # lower limit
fit$mode+1.645*se  # upper
```
Hence the 90% interval estimate for $\text{logit}(\eta)$ is (-7.28,-6.36) and for $\log K$ is (5.67,9.49).

### 5.7 Monte Carlo Method for Computing Integrals
A second general approach for summarizing a posterior is based on **simulation**. Suppose the posterior $g(\theta|y)$ and we are interested in *learning about a function $h(\theta)$*. The post mean of the function is given by:
$$
\mathbb{E}[h(\theta)|y] = \int h(\theta) g(\theta|y)d\theta.
$$
Suppose we can simulate an independent sample $\theta^1,...,\theta^m$ from the post. Then the Monte Carlo estimate at the post mean is given by:
$$
\bar h = \frac{\sum_{j=1}^m h(\theta^j)}{m}
$$
The associated simulation standard error of this estimate is:
$$
se_{\bar h} = \sqrt{\frac{\sum_{j=1}^m (h(\theta^j)-\bar h)^2}{(m-1)m}} = \frac{\bar \sigma_{h}}{m}
$$

The Monte Carlo approach is effective if *simulated samples are available from the exact posterior*. In the example of section 2.4, the proportion of heavy sleepers $p$ was our interest. Then suppose we are now interested in the posterior mean of $p^2$ (the predictive prob that 2 students in a future sample will be heavy sleepers). Then we can estimate by:
```{r}
p = rbeta(1000, 14.26, 23.19) # beta(14.26, 23.19) = posterior
est = mean(p^2)
se = sd(p^2)/sqrt(1000)
c(est,se)
```
Then, the Monte Carlo estimate at $\mathbb{E}[p|data]$ is 0.150 with se of 0.002.


### 5.8 Rejection Sampling
If we have a familiar functional form of posterior, we can simulate by an origial `R` function. However, in most of the cases, that is not ture (e.g. the beta-binomial example). A general-purpose algorithm for simulating random draws from a given distribution is **rejection sampling**.

Suppose we want to produce an independent sample from a posterior density $g(\theta|y)$ where the normalizing constant may not be known.   
**Step 1**: find another density $p(\theta)$ s.t.:  

- Easy to simulate from $p$.
- $p$ resembles $g$ in terms of location and spread.
- For all $\theta$ and a constant $c$, $g(\theta|y) \leq cp(\theta)$.

Suppose we now have $p$, then the rejection sampling technique is as follows:

1. Simulate $\theta$ from $p$ and a uniform random $u$ from  $U(0,1)$ independently.
2. If $u \leq g(\theta|y)/(cp(\theta))$ then accept $\theta$; otherwise reject it.
3. Repeat 1 and 2 until we get sufficent amount of accepted $\theta$.

As can be seen, in step 2, the *acceptance rate* of the algorithm is given by $g(\theta|y)/(cp(\theta))$, hence the higher the rate, the more efficient algorithm (i.e. $p$ and $g$ is similar). 

We consider using the technique for **beta-binamial** example where $(\text{logit}(\eta),\log K)$. One choice for $p$ might be a bivariate normal density with mean and variance given as outputs of the function `laplace`. Although this density does resemble the post but the ratio likely would not be bounded since the normal density has relatively sharp tails. A better choice is a *multivariate t* with mean and scale matrix chosen to match the post and a small degrees of freedom (to be heavy tails).  

In Section 5.5 we applied the Laplace method to find the post mean and cov matrix of $\theta=(\text{logit}(\eta),\log K)$. If the output of `laplace` is `fit` then `fit$mode` and `fit$var` are mode and covariance matrix. Thus, we set the function $p$ as a multivariate t with location `fit$mode` and scale  `2 fit$var` and 4 degrees of freedom. This choice is valid since it satisfies the 3 requests. But we must find $c$ s.t.
$$
g(\theta|y) \leq c p(\theta), \forall \theta, 
$$
or equivalently, by setting $d=\log c$
$$
\log ( g(\theta|y) ) - \log(p(\theta)) \leq d ,\forall \theta.
$$
Hence we want to *maximize $\log ( g(\theta|y) ) - \log(p(\theta))$* over all $\theta$. We here define `betabinT` to compute $d$. 
```{r}
betabinT = function(theta, datapar){
  data = datapar$data  # data
  tpar = datapar$par # params for t density
  d = betabinexch(theta, data) - dmt(theta,mean = c(tpar$m),
                                     S=tpar$var,df=tpar$df,log=TRUE) # compute the d
  # dmt: Computes the density of a multivariate t distribution
  # mean, S = cov matrix, df = degrees of freedom
  return(d) 
}
```
Also define the parameters:
```{r}
tpar = list(m=fit$mode, var=2*fit$var, df = 4)
datapar = list(data=cancermortality, par=tpar)
```
The function `laplace` finds the mode (i.e. the maximum) of this function.
```{r}
start=c(-6.9, 12.4)
fit1 = laplace(betabinT,start,datapar)
fit1$mode
```
We find the maximum value $d$ occurs at the value $\theta = (-6.889,12.422)$.  The desired value $d$ can be obtanied by
```{r}
betabinT(fit1$mode, datapar)
```
We now apply the rejection sampling. ()
```{r}
n = 10000 # number of possible sampling
dmax = betabinT(fit1$mode, datapar); # dmax is obtained above = -569.2829
theta = rmt(n, mean=c(tpar$m), S=tpar$var,df=tpar$df)  # step1: generate \theta samples
lf = c(1:dim(theta)[1])
for(j in 1:dim(theta)[1]) lf[j]=betabinexch(theta[j,],cancermortality) # compute log(g(\theta|y))
lg = dmt(theta,mean=c(tpar$m), S=tpar$var,df=tpar$df, log=T) # copute log p(theta)
prob=exp(lf-lg-dmax)  # acceptance ratio
theta = theta[runif(n) < prob,] # just pick accepted theta
dim(theta)
```
We see that 2834/10000 = 0.28 is our acceptance ratio.We can do the same thing as follows:
```{r}
theta1 = rejectsampling(betabinexch, tpar, -569.2829, 10000, cancermortality)
dim(theta1)
```

We plot the result. As expected most of the points falls inside of the contour.
```{r}
mycontour(betabinexch, c(-8,-4.5,3,17),cancermortality,
          xlab="logit eta", ylab = "log K")
points(theta[,1],theta[,2])
```


### 5.9 Importance Sampling
#### 5.9.1 Introduction
Suppose we want to solve a problem of *computing an integral in Bayesian inference*. Since the normalizing const is not known in many cases, the post mean of the function $h(\theta)$ will be given by the ratio of integrals:
$$
E[h(\theta)|y] = \frac{\int h(\theta)g(\theta|y)f(y|\theta)d\theta}{\int g(\theta)f(y|\theta)d\theta}
$$
If we were able to simulate $\{\theta^j\}$ directly, then we could approx this expected value but if not the case, suppose instead that we can make a density $p$ which we can simulate and that approx the post $g$. 
$$
E[h(\theta)|y] = \frac{\int h(\theta)g(\theta|y)f(y|\theta)d\theta}{\int g(\theta)f(y|\theta)d\theta}\\
= \frac{\int h(\theta) \frac{g(\theta|y)f(y|\theta)}{p(\theta)}p(\theta) d\theta}{\int \frac{g(\theta|y)f(y|\theta)}{p(\theta)}p(\theta)d\theta}\\
= \frac{\int h(\theta) w(\theta)p(\theta) d\theta}{\int w(\theta)p(\theta)d\theta}
$$
where $w(\theta) = \frac{g(\theta|y)f(y|\theta)}{p(\theta)}$ is the *weight function*. If we have samples $\{\theta^j\}$ from $p$, then the importance sampling esitmate of the post mean is:
$$
\bar h_{IS} = \frac{\sum_{j}h(\theta^j)w(\theta^j)}{\sum_jw(\theta^j)}
$$

This is called *importance sampling estimate* because we are sampling values of $\theta$ that are important for the integral. The simulation standard error is estimated by:
$$
se_{\bar h_{IS}} = \frac{\sqrt{\sum_{j}((h(\theta^j)-\bar h_{IS})w(\theta^j))^2}}{\sum_jw(\theta^j)}
$$
This comes from the Delta method: suppose the variable of ratio $\theta = \frac{E[Y]}{E[X]}$ then the variance of $\hat \theta$ is approximately:
$$
Var(\hat \theta) = \frac{1}{K}\frac{\sum_{j=1}^K (Y_j-\hat \theta X_j)^2}{K \bar X^2}
$$
where $\bar X$ and $\bar Y$ are the sample means for $X$ and $Y$ and $\hat \theta = \bar Y/\bar X$. Here we have, $X=w$ and $Y = hw$, thus,
$$
Var(\bar h_{IS}) = \frac{1}{K}\frac{\sum_{j}[h(\theta^j)w(\theta^j)- \bar h_{IS} w(\theta^j)]^2}{K [\frac{1}{K}\sum_jw(\theta^j)]^2}\\
= \frac{\sum_{j}[(h(\theta^j)- \bar h_{IS})w(\theta^j) ]^2}{[\sum_jw(\theta^j)]^2}
$$

The main issue of the importance sampling is to *design a good $p$*. 

Again we cosider the problem of **estiamting the post mean of a function $\theta_2=\log K$ conditional on a value of $\theta_1 = \text{logit} (\eta)$**. The post density of $\theta_2|\theta_1$ is given by:
$$
g_1(\theta_2|data,\theta_1) \propto \frac{K}{(1+K)^2}\prod_{j=1}^{20}\frac{B(K\eta+y_j,K(1-\eta)+n_j-y_j)}{B(K\eta,K(1-\eta))}\\
\iff \log(g_1(\theta_2|data,\theta_1)) \propto \log K - 2\log(1+K) \\+ \sum_{j=1}^{20}[\log (B(K\eta+y_j,K(1-\eta)+n_j-y_j)) - \log(B(K\eta,K(1-\eta)))]
$$
where $\eta = \frac{\exp \theta_1}{1+\exp \theta_1}$ and $K = \exp \theta_2$. (See Section 5.4). The following funciton computes the post density conditional on $\theta_1 = -6.819793$.
```{r}
betabinexch.cond = function(log.K, data){
  eta = exp(-6.819793)/(1+exp(-6.819793))
  K = exp(log.K)
  y = data[,1]; n = data[,2]; N = length(y)
  logf = 0*log.K
  for (j in 1:N){
    logf = logf + lbeta(K*eta+y[j], K*(1-eta)+n[j]-y[j]) - lbeta(K*eta, K*(1-eta))
  }
  val = logf + log.K - 2 * log(1+K)
  return(exp(val-max(val)))  # retrun the value of density (not log)
}
```

To compute the mean of $\log K$, suppose $p$ be normal with mean 8 and std 2. 
```{r}
I = integrate(betabinexch.cond, 3, 16, cancermortality)  # compute the normalization constant
par(mfrow = c(2,1)) # subplot
curve(betabinexch.cond(x, cancermortality)/I$value, from = 3, to = 16,
      ylab = "Density", xlab = "log K", lwd = 3, main = "Densities")
curve(dnorm(x,8,2),add=T)
legend("topright", legend = c("Exact (g)","Normal (p)"), lwd=c(3,1))
curve(betabinexch.cond(x, cancermortality)/I$value/dnorm(x, 8,2), from = 3, to = 16, 
      ylab="Weight",xlab="log K", main = "Weight = g/p")
```
Although the normal $p$ resembles the exact post $g$, the post density has a flatter right tail than the proposal $p$ thus the weight function $w$ becomes unbounded for large $\log K$. Thus we now use *t distribution with mean 8, scale 2, and 2 degrees of freedom*. 
```{r}
I = integrate(betabinexch.cond, 3, 16, cancermortality)  # compute the normalization constant
par(mfrow = c(2,2)) # subplot
# for nomal
curve(betabinexch.cond(x, cancermortality)/I$value, from = 3, to = 16,
      ylab = "Density", xlab = "log K", lwd = 3, main = "Densities")
curve(dnorm(x,8,2),add=T)
legend("topright", legend = c("Exact (g)","Normal (p)"), lwd=c(3,1))
curve(betabinexch.cond(x, cancermortality)/I$value/dnorm(x, 8,2), from = 3, to = 16, 
      ylab="Weight",xlab="log K", main = "Weight = g/p")
# for t distribution
curve(betabinexch.cond(x, cancermortality)/I$value, from = 3, to = 16,
      ylab = "Density", xlab = "log K", lwd = 3, main = "Densities")
curve(1/2*dt(x-8,df=2),add=TRUE) # t dist with mean 8, scale 2, and 2 degrees of freedom
legend("topright", legend = c("Exact (g)","T(2) (p)"), lwd=c(3,1))
curve(betabinexch.cond(x, cancermortality)/I$value/(1/2*dt(x-8,df=2)), from = 3, to = 16, 
      ylab="Weight",xlab="log K", main = "Weight = g/p")
```
We observe t dist is more attactive.

#### 5.9.2 Using a Multivariate t as a Proposal Density
Here we consider the proposal $p$ as a multivariate t distribution. `impsampling {LearnBayes}` implements importance sampling to compute the posterior mean of a function using a multivariate t proposal density. 
We here consider the posterior mean of $\log K$, our function $h$ is the second component of $\theta$.
```{r}
theta = rmt(n, mean = c(tpar$m), S = tpar$var, df = tpar$df)#sample theta using multi-t dist
lf = matrix(0,c(dim(theta)[1],1)) 
for (j in 1:dim(theta)[1]) {lf[j]=betabinexch(theta[j,],cancermortality)} # log g(theta|y): posterior
H = theta[,2]  # function h
lp = dmt(theta, mean = c(tpar$m),  S = tpar$var, df = tpar$df, log=T) # log p
md = max(lf - lp) # compute the max difference
wt = exp(lf-lp-md) # weight vector: normalize with the max value
est = sum(wt * H)/sum(wt)
SEest = sqrt(sum((H - est)^2 * wt^2))/sum(wt)  # se
cbind(est, SEest)
```
We can do the same with `impsampling`:
```{r}
tpar = list(m=fit$mode, var=2*fit$var, df = 4)
myfunc=function(theta){ # funtion h: return the second component
  return(theta[2])
}
s = impsampling(logf = betabinexch, tpar = tpar, h = myfunc, n = 10000, data = cancermortality)
cbind(s$est, s$se)
```
We also check wheather the weight $w$ is bounded. As can be seen, there are no extreme weights.
```{r}
hist(wt, main = "Histogram of Weight")
```

### 5.10 Sampling Importance Resampling
Sampling importance resampling (SIR) is a *weighted bootstrap procedure*. Suppose we obtain $m$ samples $\{\theta^j\}$ from the proposal density $p$ and compute the weight: $w(\theta^j) = g(\theta^j|y)/p(\theta^j)$. Convert these weights to probability by:
$$
p^j = \frac{w(\theta^j)}{\sum_jw(\theta^j)}
$$
We can implement this algorithm by using `sampling` of `R` function. As before, we first generate samples $\theta$ from the proposal density $p$. Then we convert the computed weights to be probability.
```{r}
tpar = list(m=fit$mode, var=2*fit$var, df = 4)  # params for multi-t dist
theta = rmt(n = 10000, mean = c(tpar$m), S = tpar$var, df = tpar$df)#sample theta using multi-t dist
lf = matrix(0,c(dim(theta)[1],1)) 
for (j in 1:dim(theta)[1]) {lf[j]=betabinexch(theta[j,],cancermortality)} # log g(theta|y): posterior
lp = dmt(theta, mean = c(tpar$m),  S = tpar$var, df = tpar$df, log=T) # log p
md = max(lf - lp) # compute the max difference
wt = exp(lf-lp-md) # weight vector: normalize with the max value

# convert to the probability
probs = wt/sum(wt)
```
Obtain samples `theta.s` from the newly defined probaility `probs` among `theta`:
```{r}
indices = sample(1:n, size = n, prob = probs, replace = T) # obtain index of theta
theta.s = theta[indices,] # obtained values
cbind(mean(theta.s[,1]),mean(theta.s[,2])) # mean of theta
```
Histogram of `theta.s`.
```{r}
par(mfrow = c(1,2))
hist(theta.s[,1], main = "logit eta")
hist(theta.s[,2], main = "log K")
```
We can do the same thing with the following funcion `sir`
```{r}
theta.s1 = sir(logf = betabinexch, tpar = tpar, n = 10000, data = cancermortality)
```

This algorithm is useful for converting simulated draws from one prob to a second prob. To see this, we here perform a **Bayesian sensitivity analysis w.r.t. the individual observations in the dataset**: we want to see the leave-one-out posterior $g(\theta|y_{(-i)})$ where we do inference without data $y_i$.
Let $\{\theta^j\}$ is a simulated sample from the full data, then sampling from $g(\theta|y_{(-i)})$  can be obtained by resampling where the sampling prob is prop to the weights:
$$
w(\theta) = \frac{g(\theta|y_{(-i)})}{g(\theta|y)} = \frac{1}{f(y_i|\theta)}\\
= \frac{B(K\eta,K(1-\eta))}{B(K\eta+y_j,K(1-\eta)+n_j-y_j)}
$$
Suppose we want to know the 5%, 50% and 95% percentile of the log precision $\log K$, and we have already simulated the sample from full data set `theta.s`.
```{r}
n = cancermortality$n; y = cancermortality$y; N = length(y)
summary=quantile(theta.s[,2],c(.05,.5,.95))  # quantiles with the full data
summary.obs=array(0,c(N,3))  # quantiles for leave-one-out data

eta = exp(theta.s[,1])/(1+exp(theta.s[,1])) ; K=exp(theta.s[,2]); 
m = length(K) # number of candidate theta
for (i in 1:N){
  weight = exp(lbeta(K*eta, K*(1-eta))
               -lbeta(K*eta+y[i], K*(1-eta)+n[i]-y[i]))
  probs = weight/sum(weight) # convert to probabilties
  indices = sample(1:m, size = m, prob = probs, replace = T) # resampling
  theta.ss = theta.s[indices,]  
  summary.obs[i,] = quantile(theta.ss[,2], c(.05,.5,.95))  # quantiles
}
```
Or we can do the same thing with `bayes.influence`:
```{r}
S = bayes.influence(theta = theta.s, data = cancermortality)
```
Plot the result.
```{r}
plot(c(0,0,0), summary, type="b", lwd = 3, xlim=c(-1,21),
     ylim = c(5,11), xlab = "Observation removed", ylab = "log K 90%quantile")
for(i in 1:N){
  lines(c(i,i,i), summary.obs[i,], type="b")
}
```

Here we observe the following:

- If data 15: (54,53637) is removed, then the precision $K$ decreases. It is reasonable since this data has a very large number of samples compared to the others.
- If data 10 or 19 is removed, then the precision $K$ increases. This is because the mortality rate of these data is relatively high compared to the others. 

