---
title: "7. Hierarchical Modeling"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

### Import the library
```{r}
library(LearnBayes) # package for the book
library(kableExtra) # for display table in HTML format
```

## 7.1 Introduction
Summary of an exchangable hierarchical model. 

## 7.2 Three Examples
We consider the following three problems.

1. Simultaneous estimation of hospital mortality rates
2. Estimating college grade point averages (Chapter 10)
3. Estiamting carrer trajectories (Chapter 11)

We here consider to consruct a prior distribution in a *hieratchical* fashion.
We begin by specifying a data distribution
$$
y \sim f(y|\theta)
$$
and the prior $\theta$ will be assined with a prior distribution with unkown hyperparameters $\lambda$.
$$
\theta \sim g_1(\theta|\lambda)
$$
The hyperparameters $\lambda$ in trun will be assigned a distribution
$$
\lambda \sim g_2(\lambda)
$$

One general way of constructing a hierarchical prior is based on the prior belief of *exchangeability*. A set of parameters $\theta = (\theta_1,\ldots,\theta_k)$ is exchangeable if the distribution of $\theta$ is unchanged if the parameter components are permuted. This implies that the belief about $\theta_j$ is the same as $\theta_h$. An exchangeable prior can be constructed by assuming the components are a ramdom sample form $g_1$:
$$
\theta_1,\ldots,\theta_k \text{ ramdom sample from } g_1(\theta|\lambda)
$$
and the unknown $\lambda$ is given by a known prior at the second stage
$$
\lambda \sim g_2(\lambda)
$$

## 7.3 Individual and Combined Estimates
**Example** heart transplant mortality data of Chapter 3.

- $e$ (Exposure): expected number of deaths
- $y_i$: observed number of deaths in hospital $i$
- $e_i$: observed number of exposures

```{r}
data(hearttransplants)
attach(hearttransplants)
```

A standard approach: $y_i$ follows a Poisson process with mean $e_i\lambda_i$ and the objective is to estimate the mortality per unit exposure $\lambda_i$. The rate $y_i/e_i$ can be viewed as an estimate of the death rate for each hospital. Below, we see the plot of $\log(e)$ vs $y/e$ for all hospitals.
```{r}
plot(log(e), y/e, xlim = c(6,9.7),xlab = "log(e)",ylab = "y/e")
text(log(e), y/e, labels = as.character(y),pos = 4)
```
We observe:  
- The esitmated rates are highly variable especially for small exposures.
- No deaths are primarily with small exposures.

Suppose we want to estimate the true mortality rate simultaneously for all hospitals $\{\lambda_i\}$. One option is simply to estimate the true rates by using the **individual death rates**.
$$
y_1/e_1,...,y_{94}/e_{94}
$$
Unfortuately, this approach wont work well, especially for the hospitals with small exposures because we have observed the rate is zero, which may induce underestimation. 

Since the individual death rates can be poor, we consider another approach. Suppose we assume that the **true mortality rates are equal**:
$$
\lambda_1 = \cdots = \lambda_{94}
$$
Under this assumption, the estimate of the mortality rates is given by
$$
\frac{\sum_{j=1}^{94}y_i}{\sum_{j=1}^{94}e_i}
$$
The problem for this pooled estimation is that this is based on the strong assumption: true mortality rates are equal for all hospitals. We want to avoid that somehow (because the rates can vary for hospitals). 

The thrid possibility is the **compromise estimate**
$$
(1-\mu) \frac{y_i}{e_i} + \mu \frac{\sum_{j=1}^{94}y_i}{\sum_{j=1}^{94}e_i}
$$
where the parameter $0<\mu<1$ determines the size of the position. 

## 7.4 Equal Mortality Rates?
Suppose we now adopt the second approach, the **true mortality rates are equal**. 
Now, $y_i$ follows Poisson$(e_i\lambda)$, and the common rate $\lambda$ is assined a standard noninformative prior of the form
$$
g(\lambda) \propto \frac{1}{\lambda}
$$
Then the posterior of $\lambda$ is given by
$$
g(\lambda|y) \propto g(\lambda) f(y|\lambda)\\
= \frac{1}{\lambda} \prod_{j=1}^{94} [\lambda^{y_j}\exp(-e_j\lambda)]\\
= \lambda^{\sum_{j=1}^{94}y_j -1}\exp(-\sum_{j=1}^{94}e_j \lambda)
$$
This can be viewed as a gamma density with paramters $\sum_{j=1}^{94}y_j$ and $\sum_{j=1}^{94}e_j$, thus the positerior is Gamma(277,294681).
```{r}
sum(y);sum(e)
```

We check this approach by assessing the *posterior predictive distribuiton*. Let $y^*$ be the number of deaths with exposure $e$ in a future sample, and it follows Poisson($e\lambda$). The unconditional distribution of $y^*$, the posterior predictive density is given by:
$$
f(y^*_i|e_i,y) = \int f_P(y_i^*|e_i\lambda)g(\lambda|y)d\lambda
$$
where $f_P(y_i^*|\lambda)$ is the Poisson sampling density with mean $\lambda$. The posterior predictive density is the likelihood of future observations based on the fitted model. 
Comparing the estimated density with the true data, we can say about the "goodness" of the model.

To test the use of the method, we consider hospital 94, whose #death = 17. First, we generate the samples for $\lambda$ by the posterior density $g(\lambda|y)$.
```{r}
lambda = rgamma(1000, shape = 277, rate = 294681)
```
Thenm the simulate draws $y_{94}^*$ can be obtained by
```{r}
ys94 = rpois(1000, e[94]*lambda)
```
The following figure shows the histogram of  $y_{94}^*$ with the actual number of deaths $y_{94}$.
```{r}
hist(ys94,breaks = seq(.5,max(ys94)+.5))
lines(c(y[94],y[94]),c(0,120),lwd=3)
```
We observe that the simulated samples are placed left of the actual number, thus this model is inconsitent: the hospital 94 has a higher mortality rate than the suggested model.

We can check the consistency for all data $y_i$ with its posterior predictive density. For each distribution, the future observation $y_i^*$ is at least as extreme as $y_i$:
$$
\min\{P(y_i^*\leq y_i),P(y_i^*\geq y_i)\}
$$
The following code computes the value for each hospital.
```{r}
lambda = rgamma(1000, shape = 277, rate = 294681)
prob.out = function(i){
  ysi = rpois(1000, e[i]*lambda)
  pleft = sum(ysi<=y[i])/1000
  pright = sum(ysi>=y[i])/1000
  return(min(pleft,pright))
}
pout = sapply(1:94,prob.out)
```
The probability vs the log exposures is as follows.
```{r}
plot(log(e), pout, ylab = "P(extreme)")
```
We see that a munber of tail probs seem small (15 are below .1). Hence this equal-rates model is inadaquete for this case. 

## 7.5 Modeling a Prior Belief of Exchangeability
First stage: the true death rates $\lambda_1,..,\lambda_{94}$ are assumed to be a random sample from a gamma$(\alpha,\alpha/\mu)$ distribution:
$$
g(\lambda|\alpha,\mu) = \frac{(\alpha/\mu)^\alpha \lambda^{\alpha-1}\exp(-\alpha\lambda/\mu)}{\Gamma(\alpha)},\lambda>0
$$
where the mean and variance are $\mu$ and $\mu^2\alpha$.  
Second Stage: the hyperparams $\alpha,\mu$ are assumed independent, with $\alpha \sim g(\alpha)$ and $\mu \sim$ inverse gamma$(a,b)$ with density of $\mu^{-a-1}\exp(-b/\mu)$.

The prior distribution induces positive correlarion between the true death rates. To illustrate this, we consider the prior for two particular rates $\lambda_1,\lambda_2$. 
We here fix $\alpha$ to $\alpha_0$. We can integrate out $\mu$ from the prior by:
$$
g(\lambda_1,\lambda_2|\alpha_0) = \int g(\lambda_1|\alpha_0,\mu) g(\lambda_2|\alpha_0,\mu) g(\mu) d\mu\\
\propto \int \frac{(\alpha_0/\mu)^{\alpha_0} \lambda_1^{\alpha_0-1}\exp(-\alpha_0\lambda_1/\mu)}{\Gamma(\alpha_0)}\frac{(\alpha_0/\mu)^{\alpha_0} \lambda_2^{\alpha_0-1}\exp(-\alpha_0\lambda_2/\mu)}{\Gamma(\alpha_0)} \mu^{-a-1}\exp(-b/\mu) d\mu\\
\propto (\lambda_1\lambda_2)^{\alpha_0-1}\int \left(\frac{1}{\mu}\right)^{(2\alpha_0+a)+1} \exp\left(-\frac{\alpha_0(\lambda_1+\lambda_2)+b}{\mu}\right)d\mu\\
\propto (\lambda_1\lambda_2)^{\alpha_0-1}\frac{1}{(\alpha_0(\lambda_1+\lambda_2)+b)^{2\alpha_0+a}} \ \ \  \text{(*from the invese gamma PDF)}
$$
We computes the log prior by the following function
```{r}
pgexchprior = function(lambda,pars){
  # lambda: true rates
  # pars: set of params
  alpha = pars[1]; a = pars[2]; b = pars[3]
  val = (alpha-1)*log(prod(lambda)) - (2*alpha+a)*log(alpha*sum(lambda)+b)
  return(val)
}
```
We assign invese gamma(10,10) for $\mu$.  We here see the contour plots of the joint density $(\lambda_1,\lambda_2)$ for the value of $\alpha_0 = 5,20,80,400$.
```{r}
alpha = c(5,20,80,400); par(mfrow=c(2,2))
for (j in 1:4) {
  mycontour(pgexchprior,c(.001,5,.001,5),c(alpha[j],10,10),
            main=paste("ALPHA = ",alpha[j]), xlab = "LAMBDA 1", ylab = "LAMBDA 2")
}
```

Here we used subjective priors, in practice vague distributions can be chosen for the hyperparameters $\mu,\alpha$. In this example, we assign:
$$
g(\mu)\propto \frac{1}{\mu},\mu > 0\\
g(\alpha) = \frac{z_0}{(\alpha+z_0)^2},\alpha > 0
$$
where $z_0$ is a median, we set 0.53.


## 7.6 Posterior Distribution
Here we assume that we use the conjugate prior, the marginal posterior of the hyperparameters can be expressed as follows (Gamma-Poisson Model)
$$
g(\alpha,\mu|y) \propto g(\alpha,\mu)g(y|\alpha,\mu)
$$
the likelihood function $g(y|\alpha,\mu)$ is given by:
$$
g(y|\alpha,\mu) = \int g(\lambda|\alpha,\mu) g(y|\lambda)d\lambda\\
= \int \frac{(\alpha/\mu)^\alpha \lambda^{\alpha-1}\exp(-\alpha\lambda/\mu)}{\Gamma(\alpha)} \frac{(e\lambda)^{y}}{y!}\exp(-e\lambda) d\lambda\\
\propto \frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)} \int \lambda^{\alpha+y-1}\exp(-(e+\frac{\alpha}{\mu})\lambda) d\lambda\\
= \frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)} \frac{\Gamma(\alpha+ y)}{(\alpha/\mu + e)^{\alpha+y}}
$$
Note that we assume that $g(y|\lambda)$ is Poisson$(e\lambda)$. Therefore, the marginal posterior is 
$$
g(\alpha,\mu|y) \propto g(\alpha,\mu)g(y|\alpha,\mu) \\
\propto \frac{1}{\mu}\frac{z_0}{(\alpha+z_0)^2} \prod_{i=1}^{94}g(y_i|\alpha,\mu)\\
\propto \frac{1}{\mu}\frac{z_0}{(\alpha+z_0)^2} \prod_{i=1}^{94}\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)} \frac{\Gamma(\alpha+ y_i)}{(\alpha/\mu + e_i)^{\alpha+y_i}}\\
$$

Moreover, the posterior of $\lambda_i$ given the hyperparameters follows gamma$(y_i+\alpha,e_i+\alpha/\mu)$, i.e.
$$
\lambda_i | \alpha, \mu, y_i \sim \text{gamma}(y_i+\alpha,e_i+\alpha/\mu)
$$
The mean is given by: 
$$
E(\lambda_i | \alpha, \mu, y_i ) = \frac{y_i+\alpha}{e_i+\alpha/\mu} = (1-B_i)\frac{y_i}{e_i}+B_i\mu\\
\text{where }B_i = \frac{\alpha}{\alpha+e_i\mu}
$$

## 7.7 Simulating from the Posterior
We here want to simulate the **all paramterer (including hyper)** from the posterior distribution, namely $g(\lambda,\alpha,\mu|y)$. However we instead consider the function as:
$$
g(\lambda,\alpha,\mu|y) \propto g(\lambda|\alpha,\mu,y) g(\alpha,\mu|y)
$$
and follow the following procedure.  
- simulate $(\alpha,\mu)$ from the marignal posterior $g(\alpha,\mu|y)$
- simulate $\lambda_i$ from the posteriror distribution conditional on $(\alpha,\mu)$, i.e. $g(\lambda_i|\alpha,\mu,y_i)$.

As usual, we first *transform the parameters to be real valued*. Hence, 
$$
\theta_1 = \log(\alpha),\theta_2 = \log(\mu)
$$
Then, the marginal posterior of the transformed params is:
$$
p(\theta_1,\theta_2|y) = g(\alpha,\mu|y) |J|
= g(\alpha,\mu|y) e^{\theta_1}e^{\theta_2} = g(\alpha,\mu|y) \alpha\mu\\
= \frac{\alpha z_0}{(\alpha+z_0)^2}\prod_{i=1}^{94}\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)} \frac{\Gamma(\alpha+ y_i)}{(\alpha/\mu + e_i)^{\alpha+y_i}}
$$
where $|J|$ is the Jacobian.

```{r}
poissgamexch = function(theta, datapar){
  # retrieve data
  y = datapar$data[,2];e = datapar$data[,1];z0 = datapar$z0
  # # hyperparams
  alpha = exp(theta[1]); mu = exp(theta[2]); beta = alpha/mu
  logf = function(y,e,alpha,beta){
    lgamma(alpha+y) - lgamma(alpha)+
      alpha*log(beta) - (alpha+y)*log(beta+e)
  }
  val = sum(logf(y,e,alpha,beta))
  val = val + log(alpha) - 2*log(alpha+z0) + log(z0)
  return(val)
}
```

Here we find the 'best' paramters by the fucntion `laplace`.
```{r}
datapar = list(data = hearttransplants, z0 = 0.53)
start = c(2,-7)
fit = laplace(poissgamexch, start, datapar)
fit
```

We see that the mode of $(\theta_1,\theta_2)$ is around (2,-7). We see the countour of the post density.
```{r}
mycontour(poissgamexch, c(0, 8, -7.3, -6.6), datapar,
          xlab = "log alpha", ylab = "log mu")
```

We obtain a simulated sample of $(\theta_1,\theta_2)$ by Gibbs sampling with fixed scale params $c_1=1,c_2=.15$. The acceptance rate is about .5.s
```{r}
start = c(4, -7)
fitgibbs = gibbs(poissgamexch, start, 1000, c(1,.15),datapar)
fitgibbs$accept
```
We plot the simulated points.
```{r}
mycontour(poissgamexch, c(0, 8, -7.3, -6.6), datapar,
          xlab = "log alpha", ylab = "log mu")
points(fitgibbs$par[,1],fitgibbs$par[,2])
```

The kernel density estimate of the precision parameter $\theta_1 = \log (\alpha)$ is given by
```{r}
plot(density(fitgibbs$par[,1], bw = .2), main = "log alpha")
```

As discussed earlier, the true mortality rates $\lambda_i$ given a hyperparameter $\alpha,\mu$ follows a gamma$(y_i+\alpha,e_i+\alpha/\mu)$. We here consider  $\lambda_1$.
```{r}
alpha = exp(fitgibbs$par[,1]); mu = exp(fitgibbs$par[,2])
lam1 = rgamma(1000, y[1]+alpha, e[1]+ alpha/mu)
```

Here we see the true data of $\log(e)$ vs $y/e$ and the 90% probability interval of the simulation for each data.
```{r}
plot(log(e), y/e, pch = as.character(y))
for (i in 1:94){
  lami = rgamma(1000, y[i]+alpha, e[i]+ alpha/mu)
  probint = quantile(lami, c(.05,.95))
  lines(log(e[i])* c(1,1), probint)
}
```


## 7.8 Posterior Inferences
Once we have obtained the rates $\lambda_i$ and the hyperparameters $\alpha, \mu$, we can use them to perform various types of inferences.
### 7.8.1 Shrinkage
The posterior mean of the true mortality rate can be approximated by:
$$
E(\lambda_i|data) \approx (1-E(B_i|data))\frac{y_i}{e_i} + E(B_i|data) \frac{\sum_{j=1}^{94}y_j}{\sum_{j=1}^{94}e_j}
$$
where $B_i = \alpha/(\alpha+e_i\mu)$ is the size of the shrinkage. We compute the all 94 shrinkage in the following:
```{r}
shrink = function(i) mean(alpha/(alpha + e[i] * mu))
shrinkage = sapply(1:94, shrink)
plot(log(e), shrinkage)
```

### 7.8.2 Comparing Hospitals
We find the best hospital: with the smallest estimated mortality rate. 
```{r}
mrate = function(i) mean(rgamma(1000, y[i] + alpha, e[i] + alpha/mu))
hospital = 1:94
meanrate = sapply(hospital, mrate)
hospital[meanrate == min(meanrate)]
```

